{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from fastdist import fastdist\n",
    "\n",
    "from get_size import get_size as mem_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem_size of theta_rows: 44,560,027\n"
     ]
    }
   ],
   "source": [
    "# get theta data\n",
    "\n",
    "with open('theta.tsv','r') as f_in:\n",
    "    theta_data = f_in.read()\n",
    "\n",
    "theta_data = theta_data.replace('*','') # very hacky, should be cleaned in data itself\n",
    "\n",
    "theta_rows = theta_data.split('\\n')\n",
    "theta_rows.pop(-1); # blank final row\n",
    "theta_rows.pop(0); # header row with topic abbreviations\n",
    "theta_rows.pop(0); # useless \"!ctsdata\" second header row\n",
    "\n",
    "# print(\"first theta data row (100-char preview):\\n%s\\n\" % theta_rows[0][:100])\n",
    "print(\"mem_size of theta_rows: %s\" % f\"{ mem_size(theta_rows) :,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(doc_ids):  15938\n",
      "mem_size of thetas: 29,161,950\n"
     ]
    }
   ],
   "source": [
    "# from theta data, get doc ids, doc full-text, and theta numbers\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "doc_ids = []\n",
    "doc_fulltext = OrderedDict() # e.g. doc_fulltext[DOC_ID]\n",
    "thetas = {} # e.g. theta[DOC_ID]\n",
    "\n",
    "for row in theta_rows:\n",
    "\n",
    "    cells = row.split('\\t') # must have been converted to TSV first!\n",
    "    doc_id, doc_text, theta_values = cells[1], cells[2], cells[3:]\n",
    "    # don't need cells[0] which would be doc_num\n",
    "    \n",
    "    doc_ids.append(doc_id)\n",
    "    doc_fulltext[doc_id] = doc_text\n",
    "    thetas[doc_id] = [ float(th) for th in theta_values ]\n",
    "\n",
    "print(\"len(doc_ids): \", len(doc_ids))\n",
    "# print(\"mem_size of doc_ids: %s\" % f\"{ mem_size(doc_ids) :,d}\")\n",
    "# print(\"mem_size of doc_fulltext: %s\" % f\"{ mem_size(doc_fulltext) :,d}\")\n",
    "print(\"mem_size of thetas: %s\" % f\"{ mem_size(thetas) :,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem_size of corpus_words_string: 15,834,636\n",
      "mem_size of corpus_words_list: 94,013,094\n",
      "mem_size of corpus_vocab: 6,294,099\n"
     ]
    }
   ],
   "source": [
    "# overall corpus string and list of all words\n",
    "\n",
    "corpus_words_string = ' '.join( doc_fulltext.values() )\n",
    "corpus_words_string.replace('  ',' ');\n",
    "\n",
    "corpus_words_list = corpus_words_string.split()\n",
    "\n",
    "corpus_vocab = list(set(corpus_words_list))\n",
    "corpus_vocab.sort()\n",
    "\n",
    "from copy import copy\n",
    "corpus_vocab_reduced = copy(corpus_vocab)\n",
    "\n",
    "# print(\"num of chars in corpus_words_string: %s\" % f\"{len(corpus_words_string):,d}\")\n",
    "# print(\"corpus_words_string preview (100-char):\\n%s\" % corpus_words_string[:100])\n",
    "print(\"mem_size of corpus_words_string: %s\" % f\"{ mem_size(corpus_words_string) :,d}\")\n",
    "# print()\n",
    "# print(\"num of words in corpus_words_list: %s\" % f\"{len(corpus_words_list):,d}\")\n",
    "# print(\"corpus_words_list preview (100-word):\\n%s\" % corpus_words_list[:100])\n",
    "print(\"mem_size of corpus_words_list: %s\" % f\"{ mem_size(corpus_words_list) :,d}\")\n",
    "# print()\n",
    "# print(\"num of words in corpus_vocab: %s\" % f\"{len(corpus_vocab):,d}\")\n",
    "# print(\"corpus_vocab preview (100-word):\\n%s\" % corpus_vocab[:100])\n",
    "print(\"mem_size of corpus_vocab: %s\" % f\"{ mem_size(corpus_vocab) :,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem_size of freq_w: 8,398,551\n"
     ]
    }
   ],
   "source": [
    "# create word frequency dictionary for words in entire corpus\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "freq_w = Counter(corpus_words_list)\n",
    "\n",
    "# print(\"frequency of word “artha”: %s\" % f\"{freq_w['artha']:,d}\")\n",
    "# print(\"frequency of word “maṅgalena”: %s\" %freq_w['maṅgalena'])\n",
    "# print(\"frequency of string “:”: %s\" % freq_w[':'])\n",
    "\n",
    "# freq_w_first_100 = {k:freq_w[k] for k in list(freq_w.keys())[:100]}\n",
    "# print(\"num of words in freq_w: %s\" % f\"{ len(freq_w) :,d}\")\n",
    "# print(\"freq_w preview (100-word):\\n%s\" % freq_w_first_100)\n",
    "\n",
    "print(\"mem_size of freq_w: %s\" % f\"{ mem_size(freq_w) :,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem_size of phi_rows: 42,971,614\n"
     ]
    }
   ],
   "source": [
    "# load phi data\n",
    "\n",
    "with open('phi.csv','r') as f_in:\n",
    "    phi_data = f_in.read()\n",
    "\n",
    "phi_data = phi_data.replace('\"','') # I think this here but not for theta because of way theta TSV was re-exported\n",
    "\n",
    "phi_rows = phi_data.split('\\n')\n",
    "phi_rows.pop(-1); # blank final row\n",
    "phi_rows.pop(0);\n",
    "\n",
    "# print(\"first phi data row (100-char preview):\\n%s\\n\" % phi_rows[0][:100])\n",
    "print(\"mem_size of phi_rows: %s\" % f\"{ mem_size(phi_rows) :,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22123/22123 [00:00<00:00, 62122.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem_size of phis: 41,199,601\n"
     ]
    }
   ],
   "source": [
    "# store phi data\n",
    "\n",
    "phis = {} # e.g., phis[WORD][TOPIC_NUM-1] = P(w|t) conditional probability\n",
    "\n",
    "for row in phi_rows:\n",
    "\n",
    "    cells = row.split(',')\n",
    "    word, phi_values = cells[0], cells[1:]\n",
    "\n",
    "    phis[word] = [ float(ph) for ph in phi_values ]\n",
    "\n",
    "# print(\"size of phis: %s\" % f\"{ get_size(phis) :,d}\")\n",
    "print(\"mem_size of phis: %s\" % f\"{ mem_size(phis) :,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(corpus_vocab):  67483\n",
      "len(corpus_vocab_reduced):  22123\n",
      "mem_size of corpus_vocab_reduced: 2,015,249\n"
     ]
    }
   ],
   "source": [
    "# note: words with freq_w < 3 were excluded from topic modeling and so will not be in phi_data, phis\n",
    "\n",
    "# the same goes for stopwords\n",
    "stopwords = ['iti', 'na', 'ca', 'api', 'eva', 'tad', 'tvāt', 'tat', 'hi', 'ādi', 'tu', 'vā'] # used in topic modeling\n",
    "\n",
    "# and here are a few errors that need to be treated in the same way\n",
    "error_words = [':', '*tat', 'eva*', '*atha', ')'] # fix in the data!\n",
    "\n",
    "# however, all of these WILL be found when going through the full-text in the theta file, which > corpus_vocab\n",
    "# so make another version that reflects this\n",
    "\n",
    "corpus_vocab_reduced = [\n",
    "    word \n",
    "    for word in corpus_vocab \n",
    "        if not (word in (stopwords + error_words) or freq_w[word] < 3)\n",
    "]\n",
    "\n",
    "# print(P_w_t['tattva'][27])\n",
    "# print('maṅgalena' in P_w_t) # False because freq_w < 3\n",
    "# print('tad' in P_w_t) # False because in stopwords\n",
    "# print(':' in P_w_t) # False because in error_words\n",
    "\n",
    "print(\"len(corpus_vocab): \", len(corpus_vocab))\n",
    "print(\"len(corpus_vocab_reduced): \", len(corpus_vocab_reduced))\n",
    "print(\"mem_size of corpus_vocab_reduced: %s\" % f\"{ mem_size(corpus_vocab_reduced) :,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) given a query document\n",
    "\n",
    "query_id = \"NBhū_104,6^1\"\n",
    "query_text = doc_fulltext[query_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15938/15938 [00:00<00:00, 153356.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for PVin_I,034,i: 0.971307\n",
      "score for ViṃśV_87,i_89,ii: 0.968265\n",
      "score for NV_478,04_478,05: 0.932906\n",
      "score for NV_205,20^2: 0.930796\n",
      "score for TriṃśBh_44,iii_44,vi: 0.927448\n",
      "score for NBh_1046,i_1046,iii: 0.926241\n",
      "score for NBh_1049,iii_1050,i: 0.926162\n",
      "score for NBh_1047,i_1047,ii: 0.925879\n",
      "score for NV_473,01_473,02: 0.923914\n",
      "score for ViṃśV_93,i_95,i: 0.922694\n",
      "mem_size of topic_candidate_vectors: 33,673,072\n"
     ]
    }
   ],
   "source": [
    "# 1) compare by topic proportions (i.e., theta values)\n",
    "\n",
    "N = 10 # number of closest docs to find\n",
    "\n",
    "query_vector = np.array(thetas[query_id])\n",
    "topic_similiarity_score = {} # e.g. topic_similiarity_score[DOC_ID] = FLOAT\n",
    "\n",
    "topic_candidate_vectors = []\n",
    "for doc_id in tqdm.tqdm(doc_ids):\n",
    "    candidate_vector = np.array(thetas[doc_id]) # dimensionality = k, number of topics\n",
    "    topic_candidate_vectors.append(candidate_vector)\n",
    "    topic_similiarity_score[doc_id] = fastdist.cosine(query_vector, candidate_vector)\n",
    "    \n",
    "sorted_results = sorted(topic_similiarity_score.items(), key=lambda item: item[1], reverse=True)\n",
    "ids_for_closest_N_docs_by_topics = [ res[0] for res in sorted_results[:N+1] ][1:] # omit first which is query itself\n",
    "\n",
    "for id in ids_for_closest_N_docs_by_topics[:20]:\n",
    "    print(\"score for %s: %f\" % (id, topic_similiarity_score[id]))\n",
    "print(\"mem_size of topic_candidate_vectors: %s\" % f\"{ mem_size(topic_candidate_vectors) :,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem_size of IDF: 9,995,307\n"
     ]
    }
   ],
   "source": [
    "# 2) compare by tf.idf scores\n",
    "\n",
    "# first calculate all IDF\n",
    "\n",
    "IDF = {} # e.g. IDF[WORD] = FLOAT for each word in vocab\n",
    "docs_containing = {} # e.g. docs_containing[WORD] = INT for each word in vocab\n",
    "\n",
    "for doc_id in doc_ids:\n",
    "    \n",
    "    doc_text = doc_fulltext[doc_id]\n",
    "    doc_words = doc_text.split()\n",
    "    unique_doc_words = list(set(doc_words))\n",
    "\n",
    "    for word in unique_doc_words:\n",
    "\n",
    "        # increment docs_containing tally\n",
    "        if word in docs_containing:\n",
    "            docs_containing[word] += 1\n",
    "        else:\n",
    "            docs_containing[word] = 1\n",
    "\n",
    "total_num_docs = len(doc_ids)\n",
    "\n",
    "for word in corpus_vocab:\n",
    "    IDF[word] = math.log(total_num_docs / docs_containing[word])\n",
    "\n",
    "print(\"mem_size of IDF: %s\" % f\"{ mem_size(IDF) :,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_freqs = sorted(freq_w.values())\n",
    "# freq_counter = Counter(all_freqs)\n",
    "# print(\"num of distinct freqs: \", len(freq_counter))\n",
    "# print(\"freq_counter: \", freq_counter)\n",
    "# keylist = list(freq_counter.keys())\n",
    "\n",
    "# from collections import OrderedDict\n",
    "# freq_counter_subset = OrderedDict()\n",
    "# for key in keylist[:100]:\n",
    "#     freq_counter_subset[key] = freq_counter[key]\n",
    "# print(\"freq_counter_subset: \", freq_counter_subset)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# labels, values = zip(*freq_counter_subset.items())\n",
    "# indexes = np.arange(len(labels))\n",
    "# width = 1\n",
    "# plt.figure(figsize=(15,4))\n",
    "# plt.bar(indexes, values, width)\n",
    "# plt.xticks(indexes + width * 0.5, labels);\n",
    "# plt.show()\n",
    "\n",
    "# # corpus_vocab_reduced = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also prepare function for calculating tf.idf vector for any given doc\n",
    "\n",
    "def get_TF_IDF_vector(doc_id):\n",
    "# returns numpy array\n",
    "    \n",
    "    doc_text = doc_fulltext[doc_id]\n",
    "    doc_words = doc_text.split()\n",
    "    unique_doc_words = list(set(doc_words))\n",
    "\n",
    "    total_doc_word_len = len(doc_words)\n",
    "\n",
    "    TF_IDF_dict = {} # e.g. TF_IDF_dict[WORD] = [FLOAT, FLOAT, FLOAT, FLOAT, ... FLOAT]\n",
    "    for word in unique_doc_words:\n",
    "        TF_d_w = doc_words.count(word) / total_doc_word_len\n",
    "        TF_IDF_dict[word] = TF_d_w * IDF[word]\n",
    "    \n",
    "    TF_IDF_vector = np.zeros( len(corpus_vocab_reduced) )\n",
    "    # e.g. TF_IDF_vector[WORD] = [0, 0, 0, ... FLOAT, 0, 0, ... FLOAT, 0, ... 0]\n",
    "    \n",
    "    for word in TF_IDF_dict.keys():\n",
    "        if word in corpus_vocab_reduced:\n",
    "            i = corpus_vocab_reduced.index(word) # alphabetical index\n",
    "            TF_IDF_vector[i] = TF_IDF_dict[word]\n",
    "    \n",
    "    return TF_IDF_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 18.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem_size of TF_IDF_candidate_vectors: 8,236,792\n"
     ]
    }
   ],
   "source": [
    "# now do actual tf.idf vector comparison on docs already selected above in topic comparison\n",
    "# relatively slow and expensive\n",
    "\n",
    "query_vector = get_TF_IDF_vector(query_id)\n",
    "TF_IDF_candidate_vectors = []\n",
    "doc_ids_for_comparison = ids_for_closest_N_docs_by_topics\n",
    "TF_IDF_comparison_scores = {} # e.g. tf_idf_score[DOC_ID] = FLOAT\n",
    "\n",
    "for doc_id in tqdm.tqdm(doc_ids_for_comparison):\n",
    "    candidate_vector = get_TF_IDF_vector(doc_id) \n",
    "    TF_IDF_candidate_vectors.append(candidate_vector)\n",
    "    TF_IDF_comparison_scores[doc_id] = fastdist.cosine(query_vector, candidate_vector)\n",
    "\n",
    "sorted_results = sorted(TF_IDF_comparison_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "print(\"mem_size of TF_IDF_candidate_vectors: %s\" % f\"{ mem_size(TF_IDF_candidate_vectors) :,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for PVin_I,034,i: 0.674471\n",
      "score for ViṃśV_87,i_89,ii: 0.020631\n",
      "score for NV_478,04_478,05: 0.073989\n",
      "score for NV_205,20^2: 0.125342\n",
      "score for TriṃśBh_44,iii_44,vi: 0.022004\n",
      "score for NBh_1046,i_1046,iii: 0.178364\n",
      "score for NBh_1049,iii_1050,i: 0.265177\n",
      "score for NBh_1047,i_1047,ii: 0.125379\n",
      "score for NV_473,01_473,02: 0.086711\n",
      "score for ViṃśV_93,i_95,i: 0.024244\n"
     ]
    }
   ],
   "source": [
    "ids_for_closest_N_docs_by_TF_IDF = [ res[0] for res in sorted_results ]\n",
    "for id in ids_for_closest_N_docs_by_topics[:20]:\n",
    "    print(\"score for %s: %f\" % (id, TF_IDF_comparison_scores[id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) compare by contextual IC score\n",
    "\n",
    "# first prepare function for calculating IC vector for any given doc\n",
    "\n",
    "def get_IC_vector(doc_id):\n",
    "# returns numpy array\n",
    "    \n",
    "    doc_text = doc_fulltext[doc_id]\n",
    "    doc_words = doc_text.split()\n",
    "    unique_doc_words = list(set(doc_words))\n",
    "\n",
    "    IC_dict = {} # e.g. IC_dict[WORD] = FLOAT\n",
    "\n",
    "    for word in unique_doc_words: # should this be doc_words instead ??\n",
    "\n",
    "        if (freq_w[word] < 3\n",
    "            or word in stopwords\n",
    "            or word in error_words\n",
    "           ): # was excluded in modeling, so neither in phi_data nor in P_w_t\n",
    "            continue\n",
    "\n",
    "        if word not in phis:\n",
    "            import pdb;pdb.set_trace()\n",
    "            # stop and add to error_words\n",
    "            \n",
    "        IC_dict[word] = 0\n",
    "        k = len(phis[word])\n",
    "        for i in range(0,k):\n",
    "            weight = thetas[doc_id][i]\n",
    "            IC_for_word_given_topic = -( weight * math.log( phis[word][i] , 2) )\n",
    "            IC_dict[word] += IC_for_word_given_topic\n",
    "    \n",
    "    IC_vector = np.zeros( len(corpus_vocab_reduced) )\n",
    "    # e.g. IC_vector = [0, 0, 0, ... FLOAT, 0, 0, ... FLOAT, 0, ... 0]\n",
    "    \n",
    "    for word in IC_dict.keys():\n",
    "        if word in corpus_vocab_reduced:\n",
    "            i = corpus_vocab_reduced.index(word) # alphabetical index\n",
    "            IC_vector[i] = IC_dict[word]\n",
    "    \n",
    "    return IC_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 22.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PVin_I,034,i', 0.539807773465966), ('NBh_1049,iii_1050,i', 0.14224065560007612), ('NV_205,20^2', 0.08552828001494603), ('NBh_1046,i_1046,iii', 0.08072412319612125), ('NV_478,04_478,05', 0.0695698144779478), ('NBh_1047,i_1047,ii', 0.06809293420775551), ('NV_473,01_473,02', 0.06443309267239383), ('ViṃśV_93,i_95,i', 0.04147626016279547), ('ViṃśV_87,i_89,ii', 0.035161954673979916), ('TriṃśBh_44,iii_44,vi', 0.032913577453843365)]\n",
      "mem_size of IC_candidate_vectors: 8,850,392\n"
     ]
    }
   ],
   "source": [
    "# now do actual IC vector comparison\n",
    "\n",
    "query_vector = get_IC_vector(query_id)\n",
    "IC_candidate_vectors = []\n",
    "doc_ids_for_comparison = ids_for_closest_N_docs_by_topics\n",
    "IC_comparison_scores = {} # e.g. IC_comparison_score[DOC_ID] = FLOAT\n",
    "\n",
    "for doc_id in tqdm.tqdm(doc_ids_for_comparison):\n",
    "    candidate_vector = get_IC_vector(doc_id) \n",
    "    IC_candidate_vectors.append(candidate_vector)\n",
    "    IC_comparison_scores[doc_id] = fastdist.cosine(query_vector, candidate_vector)\n",
    "\n",
    "sorted_results = sorted(IC_comparison_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "print(sorted_results[:20])\n",
    "\n",
    "print(\"mem_size of IC_candidate_vectors: %s\" % f\"{ mem_size(IC_candidate_vectors) :,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 463.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NBh_1047,i_1047,ii', 0.16742493175614195), ('TriṃśBh_44,iii_44,vi', 0.05390835579514825), ('PVin_I,034,i', 0.041189931350114416), ('NBh_1046,i_1046,iii', 0.038525963149078725), ('NV_473,01_473,02', 0.021078735275883446), ('NV_205,20^2', 0.014814814814814815), ('NV_478,04_478,05', 0.010484927916120577), ('ViṃśV_93,i_95,i', 0.010126582278481013), ('NBh_1049,iii_1050,i', 0.009389671361502348), ('ViṃśV_87,i_89,ii', 0.006329113924050633)]\n",
      "mem_size of SM_ratio_candidate_fulltexts: 11,258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 4) compare by SequenceMatcher.ratio score\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "# SequenceMatcher.ratio()\n",
    "#     returns a similarity score between input strings as a float in [0,1]\n",
    "#     = 2.0*M / T\n",
    "#     where M is the number of matches\n",
    "#     and T is the total number of elements in both sequences\n",
    "# <https://docs.python.org/3/library/difflib.html#difflib.SequenceMatcher.ratio>\n",
    "\n",
    "query_fulltext = doc_fulltext[query_id]\n",
    "SM_ratio_candidate_fulltexts = []\n",
    "doc_ids_for_comparison = ids_for_closest_N_docs_by_topics\n",
    "SM_ratio_comparison_scores = {} # e.g. SM_ratio_comparison_scores[DOC_ID] = FLOAT\n",
    "\n",
    "for doc_id in tqdm.tqdm(doc_ids_for_comparison):\n",
    "    candidate_fulltext = doc_fulltext[doc_id]\n",
    "    SM_ratio_candidate_fulltexts.append(candidate_fulltext)\n",
    "    SM_ratio_comparison_scores[doc_id] = SequenceMatcher(a=query_fulltext, b=candidate_fulltext).ratio()\n",
    "\n",
    "sorted_results = sorted(SM_ratio_comparison_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "print(sorted_results[:100])\n",
    "\n",
    "print(\"mem_size of SM_ratio_candidate_fulltexts: %s\" % f\"{ mem_size(SM_ratio_candidate_fulltexts) :,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = {\n",
    "    \n",
    "}\n",
    "import json\n",
    "vector_json_fn = \"vectors.json\"\n",
    "with open(vector_json_fn,'w') as f_out:\n",
    "    json_object = {}\n",
    "    json_object[\"vectors\"] = accepted_ngrams # updated\n",
    "    json.dump(json_object, f_out, indent=4, ensure_ascii=False)\n",
    "                \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
